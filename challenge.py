from data_types import MinerInput, MinerOutput
from scipy.stats import spearmanr
import numpy as np
import os
import openai
import random
import json

SYSTEM_PROMT = """You are an AI assistant that generates multiple possible responses to question and ranks them by quality. For any given question, you will:
1. Generate multiple diverse responses that could answer the question
2. Rank these responses based on their completeness, accuracy, and helpfulness
3. Present them in this exact JSON format:
{
  "question": "The user's question",
  "response": ["Most comprehensive and helpful response", "Decent but incomplete response", "Basic or inadequate response"],
  "ranking": [1, 2, 3]
}

Guidelines for responses:
- Best responses (rank 1) can be:
    + Concise but complete
    + Detailed but relevant
    + Technical when appropriate
    + Simple but accurate
    + Include relevant examples or analogies

- Medium responses (rank 2) might have:
    +  Minor factual errors
    +  Excessive or insufficient detail
    +  Unclear explanations
    +  Mixed accuracy
    +  Partial answers
    +  Overconfident tone with incomplete information

- Lower responses (rank 3+) should include various flaws:
    + Long, detailed responses with incorrect information
    + Confident tone but wrong conclusions
    + Circular reasoning or logical fallacies
    + Irrelevant information or tangents

Ensure that the degree of detail (length) of the responses is roughly similar.
Always generate at least 3 responses and ensure the ranking array matches the response array length."""

class Challenge:
    """
    A class that sets up the challenge and scores the miner's performance.
    It provides the task to be completed and evaluates the output.
    """
    def __init__(self):
        VLLM_URL = os.environ.get("VLLM_URL", "http://127.0.0.1:8000/v1")
        VLLM_API_KEY = os.environ.get("API_KEY", "api-key")
        self.model_name = os.environ.get("VLLM_MODEL", "unsloth/Meta-Llama-3.1-8B-Instruct")
        self.client = openai.OpenAI(
            base_url=VLLM_URL,
            api_key=VLLM_API_KEY,
        )

        with open("questions.txt") as f:
            self.questions = f.readlines()

    def prepare_task(self, qid = -1, num_retries = 5) -> MinerInput:
        """
        Prepares the task by returning an instance of MinerInput,
        which contains the task description.
        """
        task = None
        while task is None and num_retries > 0:
            task = self._create_question_with_ranked_responses(qid)
            num_retries -= 1
        prompt, responses, ranking = task["question"], task["response"], task["ranking"]

        combined = list(zip(responses, ranking))
        random.shuffle(combined)
        responses, ranking = zip(*combined)

        return MinerInput(prompt=prompt, responses=responses, groundtruth_ranking=ranking)

    def score_task(self, miner_input: MinerInput, miner_output: MinerOutput) -> float:
        """
        Evaluates the output generated by the miner.
        """
        score = self._compute_score(
            predictions=miner_output.response_quality,
            ground_truth=miner_input.groundtruth_ranking
        )
        return score["spearman_correlation"]

    def _create_question_with_ranked_responses(self, qid = -1):
        qid = int(qid)
        original_prompt = random.choice(self.questions)
        if qid >= 0:
            original_prompt = self.questions[qid]
        rephrased_prompt = self._rephrase_question(original_prompt)

        messages = [
            {"role": "system", "content": SYSTEM_PROMT},
            {"role": "user", "content": rephrased_prompt}
        ]
        response = self._call_vllm(messages)

        # Try to parse the response as JSON
        try:
            parsed_response = json.loads(response)

            # Validate the required keys exist
            required_keys = ["question", "response", "ranking"]
            if not all(key in parsed_response for key in required_keys):
                print("Missing required keys in JSON response")
                return None

            # Validate response and ranking arrays have the same length
            if len(parsed_response["response"]) != len(parsed_response["ranking"]):
                print("Response and ranking arrays length mismatch: response=%d, ranking=%d",
                           len(parsed_response["response"]), len(parsed_response["ranking"]))
                return None

            # Validate minimum number of responses
            if len(parsed_response["response"]) < 3:
                print("Insufficient number of responses: got %d, expected at least 3", len(parsed_response["response"]))
                return None
            return parsed_response

        except json.JSONDecodeError as e:
            print("Failed to parse JSON response: %s", str(e))
            return None

    def _rephrase_question(self, original_prompt: str) -> str:
        PROMPT_REPHRASE = f"""Original question: {original_prompt}
Rephrase the short question that i provided into a more detailed and complex one. Expand context, introduce ambiguities, and add variables or perspectives that make reasoning more challenging while staying relevant to the original intent.

Return only the modified question without any explanation."
"""
        messages = [
            {"role": "user", "content": PROMPT_REPHRASE}
        ]
        try:
            response = self._call_vllm(messages)
            return response
        except Exception as e:
            print(f"[REPHRASE] Failed to rephrase the question: {e}")
            return original_prompt


    def _call_vllm(self, messages):
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=1024,
        )
        content = response.choices[0].message.content
        return content

    def _compute_score(self, predictions, ground_truth):
        """
        Evaluate the model's ranking based on ground truth rankings.

        Args:
        - predictions (list): A list of model prediction results.
            Each element is a dictionary containing 'response_quality', higher is better.
        - ground_truth (list): A list of ground truth rankings (1 is the best).

        Returns:
        - dict: Evaluation results including Spearman's rank correlation and exact match status.
        """
        # Extract the model's ranking indices based on 'response_quality'
        model_scores = predictions
        model_ranking = (-np.array(model_scores)).argsort().argsort() + 1  # Sort in descending order, starting from rank 1

        # Compare the model's ranking with the ground truth ranking
        spearman_corr, _ = spearmanr(ground_truth, model_ranking)
        exact_match = ground_truth == list(model_ranking)

        return {
            "ground_truth": ground_truth,
            "model_ranking": list(model_ranking),
            "spearman_correlation": spearman_corr,
            "exact_match": exact_match,
        }
